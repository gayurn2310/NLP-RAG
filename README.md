# ğŸ“š NLP Textbook RAG Chatbot

This project is an end-to-end **Retrieval-Augmented Generation (RAG) chatbot** that answers questions **only from an NLP textbook PDF**.  
It has two main parts:
- **Frontend (React + Vite + Tailwind + DaisyUI)**: A chat interface for users to ask questions.
- **Backend (FastAPI + LangChain + Chroma + Groq + HuggingFace)**: A RAG pipeline that loads the textbook, embeds it, stores it in a vector DB, retrieves relevant chunks, and uses Groqâ€™s LLaMA model to generate answers.

---

## âœ¨ Features
### Frontend
- ğŸ“± Chat-like interface (with DaisyUI components)  
- ğŸ¤– Displays answers from the NLP textbook  

### Backend
- ğŸ“„ Loads and chunks **NLP textbook PDF**  
- ğŸ” Stores embeddings in **Chroma vector database**  
- ğŸ§  Uses **HuggingFace MiniLM** embeddings  
- âš¡ Queries **Groq-hosted LLaMA 3.3 70B** for answer generation  
- âŒ Rejects out-of-scope questions (only answers from the textbook)  
- ğŸŒ Exposes a `/ask` FastAPI endpoint  

---

## ğŸ› ï¸ Tech Stack
- **Frontend**: React, Vite, TailwindCSS, DaisyUI  
- **Backend**: FastAPI, LangChain, HuggingFace, Chroma, Groq API  
- **Vector DB**: Chroma (local persistence)  
- **Models**:
  - Embeddings: `all-MiniLM-L6-v2` (HuggingFace)  
  - LLM: `llama-3.3-70b-versatile` (Groq)  

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/gayurn2310/NLP-RAG.git
cd NLP-RAG
```
### 2ï¸âƒ£ Environment Variables Setup
Create a .env file in /backend with:
```bash
GROQ_API_KEY=your_groq_api_key_here
```

### 3ï¸âƒ£ Backend Setup
``` bash
cd backend
uvicorn model.main:app --reload --port 8000
```
Backend will be live at http://localhost:8000.

### 4ï¸âƒ£ Frontend Setup
``` bash
cd frontend
npm install
npm run dev
```
The frontend should be running at http://localhost:5173.
